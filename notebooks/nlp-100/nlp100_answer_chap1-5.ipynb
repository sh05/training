{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 00\n",
    "smt = \"stressed\" \n",
    "ans = \"\"\n",
    "for i in range(len(smt)):\n",
    "    ans += smt[-i - 1]\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smt[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1\n",
    "smt = \"パタトクカシーー\"\n",
    "''.join([smt[i] for i in range(len(smt)) if i % 2==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smt = \"パタトクカシーー\"\n",
    "smt[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2\n",
    "smt1 = \"パトカー\"\n",
    "smt2 = \"タクシー\"\n",
    "comb = \"\"\n",
    "for p, t in zip(smt1, smt2):\n",
    "    comb += p + t\n",
    "comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2\n",
    "smt1 = \"パトカー\"\n",
    "smt2 = \"タクシー\"\n",
    "''.join([p + t for p, t in zip(smt1, smt2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3\n",
    "smt = \"Now I need a drink, alcoholic of course, after the heavy lectures involving quantum mechanics.\"\n",
    "[len(w) - w.count(',') - w.count('.') for w in smt.split(' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4\n",
    "smt = \"Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can.\"\n",
    "dic = {}\n",
    "target_index = [1, 5, 6, 7, 8, 9, 15, 16, 19]\n",
    "for i, w in enumerate(smt.split(' ')):\n",
    "    if i + 1 in target_index:\n",
    "        dic[i + 1] = w[0]\n",
    "    else:\n",
    "        dic[i + 1] = w[:2]\n",
    "dic    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5\n",
    "def get_n_gram(n, smt):\n",
    "    words = smt.split(' ')\n",
    "    return  [smt[i:i+n] for i in range(len(smt) - n + 1)], [' '.join(words[i:i+n]) for i in range(len(words) -n + 1)]\n",
    "\n",
    "get_n_gram(3, \"I am an NLPer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6\n",
    "smt1 = \"paraparaparadise\"\n",
    "smt2 = \"paragraph\"\n",
    "X = set()\n",
    "for i in range(len(smt1) - 2 + 1):\n",
    "    X.add(smt1[i:i+2])\n",
    "Y = set()\n",
    "for i in range(len(smt2) - 2 + 1):\n",
    "    Y.add(smt2[i:i+2])\n",
    "    \n",
    "print(X | Y)\n",
    "print(X & Y)\n",
    "print(X - Y)\n",
    "print('se' in (X and Y))\n",
    "print('se' in (X or Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7\n",
    "def get_template(x, y, z):\n",
    "    return \"{}時の{}は{}\".format(x, y, z)\n",
    "\n",
    "get_template(12, '気温', 22.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8\n",
    "class Coder:\n",
    "    def __init__():\n",
    "        pass\n",
    "    \n",
    "    def encode(smt):\n",
    "        code = \"\"\n",
    "        for i in range(len(smt)):\n",
    "            if smt[i] .isalpha() and smt[i].islower():\n",
    "                code += chr(219 - ord(smt[i]))\n",
    "            else:\n",
    "                code += smt[i]\n",
    "        return code\n",
    "    \n",
    "    def decode(code):\n",
    "        stm = \"\"\n",
    "        for i in range(len(code)):\n",
    "            if code[i] .isalpha() and code[i].islower():\n",
    "                smt += chr(219 - ord(code[i]))\n",
    "            else:\n",
    "                smt += code[i]\n",
    "        return smt\n",
    "\n",
    "coder = Coder\n",
    "smt =  \"I couldn't believe that\"\n",
    "code = coder.encode(smt)\n",
    "desmt = coder.encode(code)\n",
    "print(smt)\n",
    "print(code)\n",
    "print(desmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9\n",
    "import random\n",
    "def feel_typoglycemia(smt):\n",
    "    typogly = []\n",
    "    for w in smt.split(' '):\n",
    "        if len(w) <= 4:\n",
    "            typogly.append(w)\n",
    "        else:\n",
    "            mid = list(w)[1:-1]\n",
    "            random.shuffle(mid)\n",
    "            typogly.append(w[0] + ''.join(mid) + w[-1])\n",
    "    return ' '.join(typogly)\n",
    "\n",
    "            \n",
    "smt = \"I couldn't believe that I could actually understand what I was reading : the phenomenal power of the human mind .\"\n",
    "feel_typoglycemia(smt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10\n",
    "!cat hightemp.txt | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10\n",
    "with open('./hightemp.txt',) as f:\n",
    "    print(len([r for r in f.read().split('\\n') if r is not '']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 11\n",
    "!cat hightemp.txt | sed \"s/\\t/\\ /g\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./hightemp.txt',) as f:\n",
    "    print([r.replace('\\t', ' ') for r in f.read().split('\\n') if r is not ''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!cat hightemp.txt | tr \"\\t\" \"\\ \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!expand -t 1 hightemp.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 12\n",
    "# !cat hightemp.txt | sed \"s/\\t/\\ /g\" | cut -f 1 -d \" \" > col1.txt\n",
    "# !cat hightemp.txt | sed \"s/\\t/\\ /g\" | cut -f 2 -d \" \" > col2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./hightemp.txt',) as f:\n",
    "    table = [r for r in f.read().split('\\n') if r is not '']\n",
    "    \n",
    "with open('col1.txt', mode='w') as f:\n",
    "    for t in table:\n",
    "        f.write(t.split('\\t')[0] + '\\n')\n",
    "with open('col2.txt', mode='w') as f:\n",
    "    for t in table:\n",
    "        f.write(t.split('\\t')[1] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ./col2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 13\n",
    "# !paste col1.txt col2.txt > cols.txt\n",
    "# !cat cols.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cols.txt', mode='w') as c:\n",
    "    with open('col1.txt') as f:\n",
    "        with open('col2.txt') as ff:\n",
    "            r1 = f.readline()\n",
    "            r2 = ff.readline()\n",
    "            c.write(r1.replace('\\n', '') + '\\t' + r2)\n",
    "            while r1:\n",
    "                while r2:\n",
    "                    r1 = f.readline()\n",
    "                    r2 = ff.readline()\n",
    "                    c.write(r1.replace('\\n', '') + '\\t' + r2)\n",
    "!cat cols.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 14\n",
    "!head -n 5 hightemp.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "with open('./hightemp.txt') as f:\n",
    "    lines = f.read()\n",
    "for l in lines.split('\\n')[:n]:\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 15\n",
    "!tail -n 5 hightemp.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "with open('./hightemp.txt') as f:\n",
    "    lines = f.read()\n",
    "for l in lines.split('\\n')[-n:]:\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 16\n",
    "!split -n 5 hightemp.txt \n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "with open('./hightemp.txt') as f:\n",
    "    obj = f.read()\n",
    "lines = [ l for l in obj.split('\\n')]\n",
    "n = 3\n",
    "ni = math.ceil(len(lines) / n)\n",
    "for i in range(0, len(lines), ni):\n",
    "    j = i + ni\n",
    "    print(len(lines[i:j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## 17\n",
    "!cat hightemp.txt | sed \"s/\\t/\\ /g\" | cut -f 1 -d \" \"  | sort | uniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open('./hightemp.txt') as f:\n",
    "    obj = f.read()\n",
    "set(row.split('\\t')[0] for row in obj.split('\\n') if not row =='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## 18\n",
    "!cat hightemp.txt | sed \"s/\\t/\\ /g\" | sort -r -k 3 -t \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open('./hightemp.txt') as f:\n",
    "    obj = f.read()\n",
    "rows = [row for row in obj.split('\\n') if not row =='']\n",
    "sorted(rows, key=lambda x:  -1 * float(x.split('\\t')[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 19\n",
    "!cat hightemp.txt | sed \"s/\\t/\\ /g\" | cut -f 1 -d \" \" | sort  | uniq -c | sort -rn -k 3 -t \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./hightemp.txt') as f:\n",
    "    obj = f.read()\n",
    "    \n",
    "rows =[row.split('\\t')[0] for row in obj.split('\\n') if not row =='']\n",
    "c_dic= {}\n",
    "for k in set(rows):\n",
    "    c_dic[k] = rows.count(k)\n",
    "sorted(c_dic.items(), key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget http://www.cl.ecei.tohoku.ac.jp/nlp100/data/jawiki-country.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## 20\n",
    "import json, gzip\n",
    "with gzip.open('jawiki-country.json.gz', 'rt') as f:\n",
    "    obj = json.loads(f.readline())\n",
    "    while(obj):\n",
    "        try:\n",
    "            obj = json.loads(f.readline())\n",
    "            if obj['title'] == \"イギリス\":\n",
    "                break\n",
    "        except:\n",
    "            obj = f.readline()\n",
    "obj['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 21\n",
    "for l in obj['text'].split('\\n'):\n",
    "    if 'Category' in l:\n",
    "        print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 22\n",
    "import re\n",
    "head_pattern = r'\\[\\[Category:'\n",
    "tail_pattern = r'\\|?\\*?\\]\\]'\n",
    "for l in obj['text'].split('\\n'):\n",
    "    if 'Category' in l:\n",
    "        l = re.sub(head_pattern, '', l)\n",
    "        print(re.sub(tail_pattern, '', l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 23\n",
    "sec1 = r'\\=\\='\n",
    "sec2 = r'\\=\\=\\='\n",
    "sec3 = r'\\=\\=\\=\\='\n",
    "for l in obj['text'].split('\\n'):\n",
    "    if re.search(sec3, l):\n",
    "        l = re.sub(sec3, '', l)\n",
    "        print('\\t\\t3. {}'.format(l))\n",
    "    elif re.search(sec2, l):\n",
    "        l = re.sub(sec2, '', l)\n",
    "        print('\\t2. {}'.format(l))\n",
    "    elif re.search(sec1, l):\n",
    "        l = re.sub(sec1, '', l)\n",
    "        print('1. {}'.format(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 23\n",
    "pattern = '=='\n",
    "for l in obj['text'].split('\\n'):\n",
    "    if pattern in l:\n",
    "        pat_by_sec = ''.join([r'=' for i in range(int(l.count('=') / 2 ))])\n",
    "        sec = len(pat_by_sec) - 1\n",
    "        tab = ''.join(['\\t' for i in range(sec - 1)])\n",
    "        print('{}{}. {}'.format(tab, sec, l.replace('=', '')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 24\n",
    "for l in obj['text'].split('\\n'):\n",
    "    if 'ファイル' in l:\n",
    "        print(l.split(':')[1].split('|')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 25\n",
    "import re\n",
    "pattern = r' = '\n",
    "basic_info = {}\n",
    "for l in obj['text'].split('\\n'):\n",
    "#     if re.search(pattern, l):\n",
    "    if pattern in l:\n",
    "        basic_info[l.split(' = ')[0].replace('|', '')] = l.split(' = ')[1]\n",
    "basic_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 26\n",
    "import re\n",
    "pattern = r' = '\n",
    "basic_info = {}\n",
    "for l in obj['text'].split('\\n'):\n",
    "    if pattern in l:\n",
    "        basic_info[l.split(' = ')[0].replace('|', '')] = l.split(' = ')[1].replace('\\'', '')\n",
    "basic_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 27\n",
    "import re\n",
    "pattern = r' = '\n",
    "med_link = r'\\[|\\]'\n",
    "\n",
    "basic_info = {}\n",
    "for l in obj['text'].split('\\n'):\n",
    "    if pattern in l:\n",
    "        val =  l.split(' = ')[1].replace('\\'', '')\n",
    "        val =  re.sub(med_link, '', val)\n",
    "        basic_info[l.split(' = ')[0].replace('|', '')] = val \n",
    "basic_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 28\n",
    "import re\n",
    "pattern = r' = '\n",
    "med_link = r'\\[|\\]'\n",
    "strong = r'\\{|\\}'\n",
    "tag = r'\\<+.*\\>'\n",
    "\n",
    "basic_info = {}\n",
    "for l in obj['text'].split('\\n'):\n",
    "    if pattern in l:\n",
    "        val =  l.split(' = ')[1].replace('\\'', '')\n",
    "        val =  re.sub(med_link, '', val)\n",
    "        val =  re.sub(strong, '', val)\n",
    "        val =  re.sub(tag, '', val)\n",
    "        basic_info[l.split(' = ')[0].replace('|', '')] = val \n",
    "basic_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 29\n",
    "import requests\n",
    "S = requests.Session()\n",
    "URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "PARAMS = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"prop\": \"imageinfo\",\n",
    "    \"iiprop\": \"url\",\n",
    "    \"titles\": \"File:\" + basic_info['国旗画像']\n",
    "}\n",
    "\n",
    "R = S.get(url=URL, params=PARAMS)\n",
    "DATA = R.json()\n",
    "PAGES = DATA[\"query\"][\"pages\"]\n",
    "for k, v in PAGES.items():\n",
    "    for kk, vv in v.items():\n",
    "        if kk == 'imageinfo':\n",
    "            print(vv[0]['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget http://www.cl.ecei.tohoku.ac.jp/nlp100/data/neko.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import MeCab\n",
    "t = MeCab.Tagger()\n",
    "with open('./neko.txt') as f:\n",
    "    text = f.read()\n",
    "with open('./neko.txt.mecab', mode='w') as f:\n",
    "    f.write(t.parse(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## 30\n",
    "doc = []\n",
    "with open('./neko.txt.mecab') as f:\n",
    "    token_list = []\n",
    "    token = f.readline()\n",
    "    while('EOS' not in token):\n",
    "        dic = {}\n",
    "        dic['surface'] = token.split('\\t')[0]\n",
    "        dic['base'] = token.split('\\t')[1].split(',')[-3]\n",
    "        dic['pos'] = token.split('\\t')[1].split(',')[0]\n",
    "        dic['pos1'] = token.split('\\t')[1].split(',')[1]\n",
    "        token = f.readline()\n",
    "        if dic['surface'] == '。':\n",
    "            doc.append(token_list)\n",
    "            token_list = []\n",
    "            continue\n",
    "        token_list.append(dic)\n",
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 31\n",
    "for s in doc:\n",
    "    for t in s:\n",
    "        if t['pos'] == '動詞':\n",
    "            print(t['surface'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 32\n",
    "for s in doc:\n",
    "    for t in s:\n",
    "        if t['pos'] == '動詞':\n",
    "            print(t['base'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 33\n",
    "for s in doc:\n",
    "    for t in s:\n",
    "        if t['pos1'] == 'サ変接続':\n",
    "            print(t['base'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 34\n",
    "for s in doc:\n",
    "    for i, t in enumerate(s):\n",
    "        if t['surface'] == 'の' and i + 1 != len(s):\n",
    "            if s[i -1]['pos'] == '名詞' and s[i +1]['pos'] == '名詞':\n",
    "                print(s[i -1]['surface'] + t['base'] + s[i +1]['surface'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 35\n",
    "max_list = []\n",
    "tmp = \"\"\n",
    "max_len = len(tmp)\n",
    "for s in doc:\n",
    "    for i, t in enumerate(s):\n",
    "        if t['pos'] == '名詞' :\n",
    "                tmp += t['surface']\n",
    "        else:\n",
    "            if len(tmp) == max_len:\n",
    "                max_list.append(tmp)\n",
    "            elif len(tmp) > max_len:\n",
    "                max_list = []\n",
    "                max_list.append(tmp)\n",
    "                max_len = len(tmp)\n",
    "            tmp = ''\n",
    "print(len(max_list[0]))\n",
    "print(max_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ./neko.txt | grep \"man\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## 36\n",
    "base_list = []\n",
    "count_dic = {}\n",
    "for s in doc:\n",
    "    for t in s:\n",
    "        base_list.append(t['base'])\n",
    "for word in set(base_list):\n",
    "    count_dic[word] = base_list.count(word)\n",
    "sorted(count_dic.items(), key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install japanize-matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## 37\n",
    "n = 10\n",
    "labels = [i[0] for i in sorted(count_dic.items(), key=lambda x: -x[1])[:n]]\n",
    "score = [i[1] for i in sorted(count_dic.items(), key=lambda x: -x[1])[:n]]\n",
    "\n",
    "plt.bar(labels, score)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 38\n",
    "all_score = [i[1] for i in sorted(count_dic.items(), key=lambda x: -x[1])]\n",
    "plt.hist(all_score, range(10, 100));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## 39\n",
    "log_idx = [math.log(i + 1) for i in range(len(count_dic.values()))]\n",
    "log_all_score = [math.log(i[1]) for i in sorted(count_dic.items(), key=lambda x: -x[1])]\n",
    "plt.scatter(log_idx, log_all_score, range(10, 100));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -f neko.txt.cabocha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import CaboCha\n",
    "c = CaboCha.Parser()\n",
    "with open('./neko.txt') as f:\n",
    "    text = f.read()\n",
    "with open('./neko.txt.cabocha', mode='w') as f:\n",
    "    for se in  [s + '。' for s in text.split('。')]:\n",
    "        f.write(c.parse(se ).toString(CaboCha.FORMAT_LATTICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 40\n",
    "class Morph:\n",
    "    def __init__(self, surface, base, pos, pos1):\n",
    "        self.surface = surface\n",
    "        self.base = base\n",
    "        self.pos = pos\n",
    "        self.pos1 = pos1\n",
    "\n",
    "doc = []\n",
    "with open('./neko.txt.cabocha') as f:\n",
    "    sentence = []\n",
    "    line = f.readline()\n",
    "    while(line):\n",
    "        while('EOS' not in line):\n",
    "            if not line.startswith('*'):\n",
    "                cols = line.split('\\t')\n",
    "                m = Morph(\n",
    "                    surface=cols[0],\n",
    "                    base=cols[1].split(',')[-3],\n",
    "                    pos=cols[1].split(',')[0],\n",
    "                    pos1=cols[1].split(',')[1],\n",
    "                )\n",
    "                sentence.append(m)\n",
    "            line = f.readline()\n",
    "        doc.append(sentence)\n",
    "        sentence = []\n",
    "        line = f.readline()\n",
    "print([t.surface for t in doc[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## 41\n",
    "class Chunk:\n",
    "    def __init__(self, morphs, dst, srcs):\n",
    "        self.morphs = morphs\n",
    "        self.dst = dst\n",
    "        self.srcs = srcs\n",
    "        \n",
    "doc = []\n",
    "with open('./neko.txt.cabocha') as f:\n",
    "    sentence = []\n",
    "    line = f.readline()\n",
    "    while(line):\n",
    "        if line.startswith('*'):\n",
    "            cols = line.split(' ')\n",
    "            # 前回のEOSの1行上のものが入らないように\n",
    "            if cols[1] != '0':\n",
    "                sentence.append(c)\n",
    "            c = Chunk(\n",
    "                morphs=[],\n",
    "                dst=int(cols[2].split('D')[0]),\n",
    "                srcs=[]\n",
    "            )\n",
    "        elif 'EOS' in line:\n",
    "            sentence.append(c)\n",
    "            #自分にかかるものを探す処理\n",
    "            for i, c in enumerate(sentence):\n",
    "                c.srcs = [idx for idx, chk, in enumerate(sentence) if chk.dst == i ]\n",
    "                \n",
    "            doc.append(sentence)\n",
    "            sentence = []\n",
    "        else:\n",
    "            cols = line.split('\\t')\n",
    "            if cols[1].split(',')[0] != \"記号\":\n",
    "                m = Morph(\n",
    "                    surface=cols[0],\n",
    "                    base=cols[1].split(',')[-3],\n",
    "                    pos=cols[1].split(',')[0],\n",
    "                    pos1=cols[1].split(',')[1],\n",
    "                )\n",
    "                c.morphs.append(m)\n",
    "        line = f.readline()\n",
    "for c in doc[7]:\n",
    "    print(c.dst, end=', ')\n",
    "    for m in c.morphs:\n",
    "        print(m.surface, end=\"\")\n",
    "    print()\n",
    "for c in doc[0]:\n",
    "    print(c.dst, end=', ')\n",
    "    for m in c.morphs:\n",
    "        print(m.surface)\n",
    "        print(m.pos)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 42\n",
    "# 全部はjupyter(chrome)が固まるので50\n",
    "for i, d in enumerate(doc[:50]):\n",
    "    for c in d:\n",
    "        if int(c.dst) == -1:\n",
    "            continue\n",
    "        for m in c.morphs:\n",
    "            if m.pos == '記号':\n",
    "                continue\n",
    "            print(m.surface, end=\"\")\n",
    "        print('\\t', end=\"\")\n",
    "        for m in d[c.dst].morphs:\n",
    "            if m.pos == '記号':\n",
    "                continue\n",
    "            print(m.surface, end=\"\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 43\n",
    "# 全部はjupyter(chrome)が固まるので50\n",
    "for i, d in enumerate(doc[:50]):\n",
    "    for c in d:\n",
    "        if int(c.dst) == -1:\n",
    "            continue\n",
    "        contain_noun = '名詞' in [m.pos for m in c.morphs]\n",
    "        contain_verb = '動詞' in [m.pos for m in d[c.dst].morphs]\n",
    "        if contain_noun and contain_verb:\n",
    "            for m in c.morphs:\n",
    "                if m.pos == '記号':\n",
    "                    continue\n",
    "                print(m.surface, end=\"\")\n",
    "            print('\\t', end=\"\")\n",
    "            for m in d[int(c.dst)].morphs:\n",
    "                if m.pos == '記号':\n",
    "                    continue\n",
    "                print(m.surface, end=\"\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!apt-get install -y graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## 44\n",
    "import random, pathlib\n",
    "from graphviz import Digraph\n",
    "\n",
    "f = pathlib.Path('nekocabocha.png')\n",
    "fmt = f.suffix.lstrip('.')\n",
    "fname = f.stem\n",
    "target_doc = random.choice(doc)\n",
    "target_doc = doc[8]\n",
    "idx = doc.index(target_doc)\n",
    "\n",
    "dot = Digraph(format=fmt)\n",
    "dot.attr(\"node\", shape=\"circle\")\n",
    "\n",
    "N = len(target_doc)\n",
    "# ノードの追加\n",
    "for i in range(N):\n",
    "    dot.node(str(i), ''.join([m.surface for m in target_doc[i].morphs]))\n",
    "\n",
    "# 辺の追加\n",
    "for i in range(N):\n",
    "    if target_doc[i].dst >= 0:\n",
    "        dot.edge(str(i), str(target_doc[i].dst))\n",
    "\n",
    "# dot.engine = \"circo\"\n",
    "dot.filename = filename\n",
    "dot.render()\n",
    "\n",
    "print(''.join([m.surface for c in target_doc for m in c.morphs]))\n",
    "print(dot)\n",
    "from IPython.display import Image, display_png\n",
    "display_png(Image(str(f)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 45\n",
    "with open(\"neko_verb.txt\", mode=\"w\") as f:\n",
    "    for s in doc:\n",
    "        for c in s:\n",
    "            if '動詞' in [m.pos for m in c.morphs]:\n",
    "                row = c.morphs[0].base\n",
    "                j_list = []\n",
    "                for i in c.srcs:\n",
    "                    if len(s[i].morphs) < 2:\n",
    "                        continue\n",
    "                    srclast = s[i].morphs[-1]\n",
    "                    if srclast.pos == '助詞':\n",
    "                        j_list.append(srclast.surface)\n",
    "                if len(j_list) > 0:\n",
    "                    j_list.sort()\n",
    "                    row += \"\\t\" +  \" \".join(j_list)\n",
    "                    f.write(row + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat neko_verb.txt | sort  | uniq -c  | sort -rn -k 3\n",
    "!cat neko_verb.txt | grep \"^する\" | sort  | uniq -c  | sort -rn -k 3\n",
    "!cat neko_verb.txt | grep \"見る\" | sort  | uniq -c  | sort -rn -k 3\n",
    "!cat neko_verb.txt | grep \"与える\" | sort  | uniq -c  | sort -rn -k 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 46\n",
    "# 出力例とは異なるが問題の仕様はこちらの方が満たしている\n",
    "for s in doc:\n",
    "    for c in s:\n",
    "        if '動詞' in [m.pos for m in c.morphs]:\n",
    "            row = c.morphs[0].base\n",
    "            j_list = []\n",
    "            c_list = []\n",
    "            for i in c.srcs:\n",
    "                if len(s[i].morphs) < 2:\n",
    "                    continue\n",
    "                srclast = s[i].morphs[-1]\n",
    "                if srclast.pos == '助詞':\n",
    "                    j_list.append(srclast.surface)\n",
    "                    c_list.append(''.join([m.surface for m in s[i].morphs]))\n",
    "            if len(j_list) > 0:\n",
    "                j_list.sort()\n",
    "                c_list.sort()\n",
    "                row += \"\\t\" +  \" \".join(j_list) + \"\\t\"+  \" \".join(c_list)\n",
    "                print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 47\n",
    "with open(\"neko_func_verb.txt\", mode=\"w\") as f:\n",
    "    for s in doc:\n",
    "        for c in s:\n",
    "            if '動詞' in [m.pos for m in c.morphs]:\n",
    "                verb = c.morphs[0].base\n",
    "                for i in c.srcs:\n",
    "                    v_head = s[i].morphs[-2:]\n",
    "                    if len(v_head) < 2:\n",
    "                        continue\n",
    "                    if v_head[0].pos1 == \"サ変接続\" and v_head[1].surface == \"を\":\n",
    "                        verb = ''.join([m.surface for m in v_head]) + verb\n",
    "                        joshi_dic = {}\n",
    "\n",
    "                        for j in c.srcs:\n",
    "                            if len(s[j].morphs) < 2:\n",
    "                                continue\n",
    "                            srclast = s[j].morphs[-1]\n",
    "                            if srclast.pos == '助詞' and srclast.surface != \"を\":\n",
    "                                joshi_dic[srclast.surface] =  ''.join([m.surface for m in s[j].morphs])\n",
    "\n",
    "                        if len(joshi_dic.keys()) > 0:\n",
    "                            joshi_list = list(joshi_dic.keys())\n",
    "                            joshi_list.sort()\n",
    "                            row = verb + \"\\t\" +  \" \".join(joshi_list) + \"\\t\" + \" \".join([joshi_dic[joshi] for joshi in joshi_list])\n",
    "                            f.write(row + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat neko_func_verb.txt | sed \"s/\\t/ /g\"| cut -f 1 -d \" \" | sort | uniq -c  | sort -rn -k 3\n",
    "!cat neko_func_verb.txt | sed \"s/\\t/+/g\"| cut -f 1,2 -d \"+\" | sed \"s/+/\\t/g\" | sort | uniq -c  | sort -rn -k 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 48\n",
    "for s in doc:\n",
    "    for c in s:\n",
    "        if \"名詞\" in [m.pos for m in c.morphs]:\n",
    "            row = \"\".join([m.surface for m in c.morphs])\n",
    "            chunk_to = c.dst\n",
    "            if chunk_to == -1:\n",
    "                continue\n",
    "            while(chunk_to != -1):\n",
    "                row += \" -> \" + \"\".join([m.surface for m in s[chunk_to].morphs])\n",
    "                chunk_to = s[chunk_to].dst\n",
    "            print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 49\n",
    "for s in doc:\n",
    "    # i < j のため最後尾は不要\n",
    "    for i, c in enumerate(s[:-1]):\n",
    "        if \"名詞\" in [m.pos for m in c.morphs] and c.morphs[-1].pos == \"助詞\":\n",
    "            # j を探す\n",
    "            for c_rest in s[i+1:]:\n",
    "                if \"名詞\" in [m.pos for m in c_rest.morphs] and c_rest.morphs[-1].pos == \"助詞\":\n",
    "                    i_clause =  \"\".join([m.surface if m.pos != \"名詞\" else \"X\" for m in c.morphs])\n",
    "                    j_clause =  \"\".join([m.surface if m.pos != \"名詞\" else \"Y\" for m in c_rest.morphs])\n",
    "                    \n",
    "                    row = i_clause\n",
    "                    chunk_to = c.dst\n",
    "                    # パス上にjが存在するか確認のためにパスを求める\n",
    "                    kkr_path = [chunk_to]\n",
    "                    while(kkr_path[-1] != -1):\n",
    "                        kkr_path.append(s[chunk_to].dst)\n",
    "                        chunk_to = s[chunk_to].dst\n",
    "                    \n",
    "                    if s.index(c_rest) in kkr_path:\n",
    "                        chunk_to = c.dst\n",
    "                        while(chunk_to != s.index(c_rest)):\n",
    "                            row += \" -> \" + \"\".join([m.surface for m in s[chunk_to].morphs])\n",
    "                            chunk_to = s[chunk_to].dst\n",
    "                        row += \" -> \" + j_clause\n",
    "                    else:\n",
    "                        row += \" | \" + j_clause\n",
    "                        chunk_to = c_rest.dst\n",
    "                        while(s[chunk_to].dst != -1):\n",
    "                            row += \" -> \" + \"\".join([m.surface for m in s[chunk_to].morphs])\n",
    "                            chunk_to = s[chunk_to].dst\n",
    "                        row += \" | \" + \"\".join([m.surface for m in s[chunk_to].morphs])\n",
    "                        \n",
    "                    print(row)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
