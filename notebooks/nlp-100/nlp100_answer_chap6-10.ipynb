{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://www.cl.ecei.tohoku.ac.jp/nlp100/data/nlp.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 50\n",
    "import re\n",
    "sentence_sep = re.compile(r'(\\.|;|:|\\?|!) ([A-Z])')\n",
    "\n",
    "with open(\"./nlp.txt\") as f:\n",
    "    txt = f.read()\n",
    "txt = re.sub(sentence_sep, r'\\1\\n\\2', txt)\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 51\n",
    "def space2return(txt):\n",
    "    sentence_sep = re.compile(r'(\\.|;|:|\\?|!)\\n([A-Z])')\n",
    "    txt = re.sub(sentence_sep, r'\\1\\n\\n\\2', txt)\n",
    "    return re.sub(r' ', r'\\n', txt)\n",
    "\n",
    "txt = space2return(txt)\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 52\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def stem_text(txt):\n",
    "    for l in txt.split('\\n'):\n",
    "        yield l + '\\t' + ps .stem(l)\n",
    "    \n",
    "for line in stem_text(txt):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget http://nlp.stanford.edu/software/stanford-corenlp-full-2018-10-05.zip\n",
    "# !unzip stanford-corenlp-full-2018-10-05.zip\n",
    "!java -cp \"./stanford-corenlp-full-2018-10-05/*\" edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,parse,lemma,ner,coref -file ./nlp.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 53\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "tree = ET.parse(\"./nlp.txt.xml\")\n",
    "root = tree.getroot()\n",
    "for token in root.iter(\"token\"):\n",
    "    print(token.find(\"word\").text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 54\n",
    "for token in root.iter(\"token\"):\n",
    "    print(token.find(\"word\").text + \"\\t\" + token.find(\"lemma\").text + \"\\t\" + token.find(\"POS\").text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 55\n",
    "for token in root.iter(\"token\"):\n",
    "    NERtag = token.find(\"NER\").text\n",
    "    if NERtag == \"PERSON\":\n",
    "        print(token.find(\"word\").text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in root.find(\"document\"):\n",
    "    print(r.tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 56\n",
    "rep_dic_list = []\n",
    "\n",
    "# 辞書作り\n",
    "for coreferences in root.findall(\"document/coreference\"):\n",
    "    for mentions in coreferences:\n",
    "        for m in mentions:\n",
    "            if \"representative\" in m.attrib:\n",
    "                rep_txt = m.find(\"text\").text\n",
    "            else:\n",
    "                tmp_dic = {}\n",
    "                tmp_dic[\"sentence\"] = m.find(\"sentence\").text\n",
    "                tmp_dic[\"start\"] = m.find(\"start\").text\n",
    "                tmp_dic[\"end\"] = m.find(\"end\").text\n",
    "                tmp_dic[\"rep_txt\"] = rep_txt\n",
    "                rep_dic_list.append(tmp_dic)\n",
    "                \n",
    "# 出力\n",
    "for s in root.iter(\"sentence\"):\n",
    "    rep_sent_list = [rd for rd in rep_dic_list if rd[\"sentence\"] == s.attrib[\"id\"]]\n",
    "    # 置換が必要な文かどうか\n",
    "    if len(rep_sent_list) == 0:\n",
    "            print(\" \".join([token.find(\"word\").text for token in s.iter(\"token\")]), end=\" \")\n",
    "    else:\n",
    "        for token in s.iter(\"token\"):\n",
    "            tid = token.attrib[\"id\"]\n",
    "            rep_token_list = [rd for rd in rep_sent_list if rd[\"start\"] == tid or rd[\"end\"] == tid]\n",
    "            \n",
    "            if len(rep_token_list) > 0:\n",
    "                # 該当は１つなので取り出す\n",
    "                rep_dic = rep_token_list[0]\n",
    "                \n",
    "                #　装飾\n",
    "                if tid == rep_dic[\"start\"]:\n",
    "                    print(\"「\" + rep_dic[\"rep_txt\"] + \" (\", end=\" \")\n",
    "                if tid == rep_dic[\"end\"]:\n",
    "                    print(\")」\", end=\" \")\n",
    "                    \n",
    "            print(token.find(\"word\").text, end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 57\n",
    "import random, pathlib\n",
    "from graphviz import Digraph\n",
    "\n",
    "f = pathlib.Path('nlp.png')\n",
    "fmt = f.suffix.lstrip('.')\n",
    "fname = f.stem\n",
    "\n",
    "dot = Digraph(format=fmt)\n",
    "dot.attr(\"node\", shape=\"circle\")\n",
    "\n",
    "sent_id = 3\n",
    "\n",
    "for sents in root.findall(f\"document/sentences/sentence[@id='{sent_id}']\"):\n",
    "    for deps in sents:\n",
    "        for dep in deps.findall(\"[@type='collapsed-dependencies']\"):\n",
    "            for token in dep:\n",
    "                gvnr = token.find(\"governor\")\n",
    "                dpnt = token.find(\"dependent\")\n",
    "                dot.node(gvnr.attrib[\"idx\"], gvnr.text)\n",
    "                dot.node(dpnt.attrib[\"idx\"], dpnt.text)\n",
    "                dot.edge(gvnr.attrib[\"idx\"], dpnt.attrib[\"idx\"])\n",
    "\n",
    "\n",
    "dot.filename = fname\n",
    "dot.render()\n",
    "\n",
    "# print(dot)\n",
    "from IPython.display import Image, display_png\n",
    "display_png(Image(str(f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 58\n",
    "for sents in root.findall(f\"document/sentences/sentence\"):\n",
    "    for deps in sents:\n",
    "        for dep in deps.findall(\"[@type='collapsed-dependencies']\"):\n",
    "            nsubj_list = []\n",
    "            for token in dep.findall(\"./dep[@type='nsubj']\"):\n",
    "                gvnr = token.find(\"governor\")\n",
    "                dpnt = token.find(\"dependent\")\n",
    "                nsubj_list.append( {\n",
    "                    (gvnr.attrib[\"idx\"], gvnr.text): (dpnt.attrib[\"idx\"], dpnt.text)\n",
    "                })\n",
    "            for token in dep.findall(\"./dep[@type='dobj']\"):\n",
    "                gvnr = token.find(\"governor\")\n",
    "                dpnt = token.find(\"dependent\")\n",
    "                dobj_tuple = (gvnr.attrib[\"idx\"], gvnr.text)\n",
    "                \n",
    "                if dobj_tuple in [list(nsubj.keys())[0] for nsubj in nsubj_list]:\n",
    "                    idx =  [list(nsubj.keys())[0] for nsubj in nsubj_list].index( dobj_tuple )\n",
    "                    jutugo = gvnr.text\n",
    "                    shugo = nsubj_list[idx][dobj_tuple][1]\n",
    "                    mokutekigo = dpnt.text\n",
    "                    print(shugo + \"\\t\" + jutugo + \"\\t\" + mokutekigo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_nest(t):\n",
    "    if isinstance(t[0], str):\n",
    "        if isinstance(t[1], str):\n",
    "            if t[0] == \"NP\":\n",
    "                print(t[1])\n",
    "            return t[1]\n",
    "        else:\n",
    "            if t[0] == \"NP\":\n",
    "                np_list = []\n",
    "                for i in t[1:]:\n",
    "                    res = search_nest(i)\n",
    "                    if isinstance(res, str):\n",
    "                        np_list.append(search_nest(i))\n",
    "                if len(np_list) > 0:\n",
    "                    print(' '.join(np_list))\n",
    "            else:\n",
    "                for i in t[1:]:\n",
    "                    search_nest(i)\n",
    "    else:\n",
    "        for i in t:\n",
    "            search_nest(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 59 \n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "\n",
    "tree = ET.parse(\"./nlp.txt.xml\")\n",
    "root = tree.getroot()\n",
    "sent_id = 30\n",
    "\n",
    "for parse in root.findall(f\"document/sentences/sentence[@id='{sent_id}']/parse\"):\n",
    "    S_str = parse.text\n",
    "    S_str = S_str.replace(\"(\", \"('\")\n",
    "    S_str = S_str.replace(\")\", \"')\")\n",
    "    S_str = S_str.replace(\" \", \"', '\")\n",
    "    S_str = S_str.replace(\"'(\", \"(\")\n",
    "    S_str = S_str.replace(\")'\", \")\")\n",
    "    exec(f\"S_tuple = {S_str[:-2]}\")\n",
    "    search_nest(S_tuple)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !wget http://www.cl.ecei.tohoku.ac.jp/nlp100/data/artist.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "\n",
    "r = redis.Redis(host=\"redis\", port=6379, db=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'fuga'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.set(\"hoge\", \"fuga\")\n",
    "hoge = r.get(\"hoge\")\n",
    "print(hoge)\n",
    "\n",
    "r.delete(\"hoge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第7章: データベース\n",
    ">artist.json.gzは，オープンな音楽データベースMusicBrainzの中で，アーティストに関するものをJSON形式に変換し，gzip形式で圧縮したファイルである．このファイルには，1アーティストに関する情報が1行にJSON形式で格納されている．JSON形式の概要は以下の通りである．\n",
    "\n",
    "|フィールド|型|内容|例|\n",
    "|----|----|----|----|\n",
    "|id|ユニーク識別子|整数|20660|\n",
    "|gid|グローバル識別子|文字列|\"ecf9f3a3-35e9-4c58-acaa-e707fba45060\"|\n",
    "|name|アーティスト名|文字列|\"Oasis\"|\n",
    "|sort_name|アーティスト名（辞書順整列用）|文字列|\"Oasis\"|\n",
    "|area|活動場所|文字列|\"United|Kingdom\"|\n",
    "|aliases|別名|辞書オブジェクトのリスト||\n",
    "|aliases[].name|別名|文字列|\"オアシス\"|\n",
    "|aliases[].sort_name|別名（整列用）|文字列|\"オアシス\"|\n",
    "|begin|活動開始日|辞書||\n",
    "|begin.year|活動開始年|整数|1991|\n",
    "|begin.month|活動開始月|整数||\n",
    "|begin.date|活動開始日|整数||\n",
    "|end|活動終了日|辞書||\n",
    "|end.year|活動終了年|整数|2009|\n",
    "|end.month|活動終了月|整数|8|\n",
    "|end.date|活動終了日|整数|28|\n",
    "|tags|タグ|辞書オブジェクトのリスト||\n",
    "|tags[].count|タグ付けされた回数|整数|1|\n",
    "|tags[].value|タグ内容|文字列|\"rock\"|\n",
    "|rating|レーティング|辞書オブジェクト||\n",
    "|rating.count|レーティングの投票数|整数|13|\n",
    "|rating.value|レーティングの値（平均値）|整数|86|\n",
    "\n",
    ">artist.json.gzのデータをKey-Value-Store (KVS) およびドキュメント志向型データベースに格納・検索することを考える．KVSとしては，LevelDB，Redis，KyotoCabinet等を用いよ．ドキュメント志向型データベースとして，MongoDBを採用したが，CouchDBやRethinkDB等を用いてもよい．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 60. KVSの構築\n",
    ">Key-Value-Store (KVS) を用い，アーティスト名（name）から活動場所（area）を検索するためのデータベースを構築せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 61. KVSの検索\n",
    ">60で構築したデータベースを用い，特定の（指定された）アーティストの活動場所を取得せよ．\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 62. KVS内の反復処理\n",
    ">60で構築したデータベースを用い，活動場所が「Japan」となっているアーティスト数を求めよ．\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 63. オブジェクトを値に格納したKVS\n",
    ">KVSを用い，アーティスト名（name）からタグと被タグ数（タグ付けされた回数）のリストを検索するためのデータベースを構築せよ．さらに，ここで構築したデータベースを用い，アーティスト名からタグと被タグ数を検索せよ．\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 64. MongoDBの構築\n",
    ">アーティスト情報（artist.json.gz）をデータベースに登録せよ．さらに，次のフィールドでインデックスを作成せよ: name, aliases.name, tags.value, rating.value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 65. MongoDBの検索\n",
    ">MongoDBのインタラクティブシェルを用いて，\"Queen\"というアーティストに関する情報を取得せよ．さらに，これと同様の処理を行うプログラムを実装せよ．\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 66. 検索件数の取得\n",
    ">MongoDBのインタラクティブシェルを用いて，活動場所が「Japan」となっているアーティスト数を求めよ．\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 67. 複数のドキュメントの取得\n",
    ">特定の（指定した）別名を持つアーティストを検索せよ．\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 68. ソート\n",
    ">\"dance\"というタグを付与されたアーティストの中でレーティングの投票数が多いアーティスト・トップ10を求めよ．\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 69. Webアプリケーションの作成\n",
    ">ユーザから入力された検索条件に合致するアーティストの情報を表示するWebアプリケーションを作成せよ．アーティスト名，アーティストの別名，タグ等で検索条件を指定し，アーティスト情報のリストをレーティングの高い順などで整列して表示せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第8章: 機械学習\n",
    ">本章では，Bo Pang氏とLillian Lee氏が公開しているMovie Review Dataのsentence polarity dataset v1.0を用い，文を肯定的（ポジティブ）もしくは否定的（ネガティブ）に分類するタスク（極性分析）に取り組む．\n",
    "\n",
    "#### 70. データの入手・整形\n",
    ">文に関する極性分析の正解データを用い，以下の要領で正解データ（sentiment.txt）を作成せよ．\n",
    ">rt-polarity.posの各行の先頭に\"+1 \"という文字列を追加する（極性ラベル\"+1\"とスペースに続けて肯定的な文の内容が続く）\n",
    "rt-polarity.negの各行の先頭に\"-1 \"という文字列を追加する（極性ラベル\"-1\"とスペースに続けて否定的な文の内容が続く）\n",
    "上述1と2の内容を結合（concatenate）し，行をランダムに並び替える\n",
    "sentiment.txtを作成したら，正例（肯定的な文）の数と負例（否定的な文）の数を確認せよ．\n",
    "\n",
    "#### 71. ストップワード\n",
    ">英語のストップワードのリスト（ストップリスト）を適当に作成せよ．さらに，引数に与えられた単語（文字列）がストップリストに含まれている場合は真，それ以外は偽を返す関数を実装せよ．さらに，その関数に対するテストを記述せよ．\n",
    "\n",
    "#### 72. 素性抽出\n",
    ">極性分析に有用そうな素性を各自で設計し，学習データから素性を抽出せよ．素性としては，レビューからストップワードを除去し，各単語をステミング処理したものが最低限のベースラインとなるであろう．\n",
    "\n",
    "#### 73. 学習\n",
    ">72で抽出した素性を用いて，ロジスティック回帰モデルを学習せよ．\n",
    "\n",
    "#### 74. 予測\n",
    ">73で学習したロジスティック回帰モデルを用い，与えられた文の極性ラベル（正例なら\"+1\"，負例なら\"-1\"）と，その予測確率を計算するプログラムを実装せよ．\n",
    "\n",
    "#### 75. 素性の重み\n",
    ">73で学習したロジスティック回帰モデルの中で，重みの高い素性トップ10と，重みの低い素性トップ10を確認せよ．\n",
    "\n",
    "#### 76. ラベル付け\n",
    ">学習データに対してロジスティック回帰モデルを適用し，正解のラベル，予測されたラベル，予測確率をタブ区切り形式で出力せよ．\n",
    "\n",
    "#### 77. 正解率の計測\n",
    ">76の出力を受け取り，予測の正解率，正例に関する適合率，再現率，F1スコアを求めるプログラムを作成せよ．\n",
    "\n",
    "#### 78. 5分割交差検定\n",
    ">76-77の実験では，学習に用いた事例を評価にも用いたため，正当な評価とは言えない．すなわち，分類器が訓練事例を丸暗記する際の性能を評価しており，モデルの汎化性能を測定していない．そこで，5分割交差検定により，極性分類の正解率，適合率，再現率，F1スコアを求めよ．\n",
    "\n",
    "#### 79. 適合率-再現率グラフの描画\n",
    ">ロジスティック回帰モデルの分類の閾値を変化させることで，適合率-再現率グラフを描画せよ．\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第9章: ベクトル空間法 (I)\n",
    ">enwiki-20150112-400-r10-105752.txt.bz2は，2015年1月12日時点の英語のWikipedia記事のうち，約400語以上で構成される記事の中から，ランダムに1/10サンプリングした105,752記事のテキストをbzip2形式で圧縮したものである．このテキストをコーパスとして，単語の意味を表すベクトル（分散表現）を学習したい．第9章の前半では，コーパスから作成した単語文脈共起行列に主成分分析を適用し，単語ベクトルを学習する過程を，いくつかの処理に分けて実装する．第9章の後半では，学習で得られた単語ベクトル（300次元）を用い，単語の類似度計算やアナロジー（類推）を行う．\n",
    "\n",
    "#### 80. コーパスの整形\n",
    ">文を単語列に変換する最も単純な方法は，空白文字で単語に区切ることである． ただ，この方法では文末のピリオドや括弧などの記号が単語に含まれてしまう． そこで，コーパスの各行のテキストを空白文字でトークンのリストに分割した後，各トークンに以下の処理を施し，単語から記号を除去せよ．\n",
    ">トークンの先頭と末尾に出現する次の文字を削除: .,!?;:()[]'\"\n",
    "空文字列となったトークンは削除\n",
    "以上の処理を適用した後，トークンをスペースで連結してファイルに保存せよ．\n",
    "\n",
    "#### 81. 複合語からなる国名への対処\n",
    ">英語では，複数の語の連接が意味を成すことがある．例えば，アメリカ合衆国は\"United States\"，イギリスは\"United Kingdom\"と表現されるが，\"United\"や\"States\"，\"Kingdom\"という単語だけでは，指し示している概念・実体が曖昧である．そこで，コーパス中に含まれる複合語を認識し，複合語を1語として扱うことで，複合語の意味を推定したい．しかしながら，複合語を正確に認定するのは大変むずかしいので，ここでは複合語からなる国名を認定したい．\n",
    "インターネット上から国名リストを各自で入手し，80のコーパス中に出現する複合語の国名に関して，スペースをアンダーバーに置換せよ．例えば，\"United States\"は\"United_States\"，\"Isle of Man\"は\"Isle_of_Man\"になるはずである．\n",
    "\n",
    "#### 82. 文脈の抽出\n",
    ">81で作成したコーパス中に出現するすべての単語tに関して，単語tと文脈語cのペアをタブ区切り形式ですべて書き出せ．ただし，文脈語の定義は次の通りとする．\n",
    "ある単語tの前後d単語を文脈語cとして抽出する（ただし，文脈語に単語tそのものは含まない）\n",
    "単語tを選ぶ度に，文脈幅dは{1,2,3,4,5}の範囲でランダムに決める．\n",
    "\n",
    "#### 83. 単語／文脈の頻度の計測\n",
    ">82の出力を利用し，以下の出現分布，および定数を求めよ．\n",
    "f(t,c): 単語tと文脈語cの共起回数\n",
    "f(t,∗): 単語tの出現回数\n",
    "f(∗,c): 文脈語cの出現回数\n",
    "N: 単語と文脈語のペアの総出現回数\n",
    "\n",
    "#### 84. 単語文脈行列の作成\n",
    ">83の出力を利用し，単語文脈行列Xを作成せよ．ただし，行列Xの各要素Xtcは次のように定義する．\n",
    "\n",
    ">f(t,c)≥10ならば，Xtc=PPMI(t,c)=max{logN×f(t,c)f(t,∗)×f(∗,c),0}\n",
    "f(t,c)<10ならば，Xtc=0\n",
    "ここで，PPMI(t,c)はPositive Pointwise Mutual Information（正の相互情報量）と呼ばれる統計量である．なお，行列Xの行数・列数は数百万オーダとなり，行列のすべての要素を主記憶上に載せることは無理なので注意すること．幸い，行列Xのほとんどの要素は0になるので，非0の要素だけを書き出せばよい．\n",
    "\n",
    "#### 85. 主成分分析による次元圧縮\n",
    ">84で得られた単語文脈行列に対して，主成分分析を適用し，単語の意味ベクトルを300次元に圧縮せよ．\n",
    "\n",
    "#### 86. 単語ベクトルの表示\n",
    ">85で得た単語の意味ベクトルを読み込み，\"United States\"のベクトルを表示せよ．ただし，\"United States\"は内部的には\"United_States\"と表現されていることに注意せよ．\n",
    "\n",
    "#### 87. 単語の類似度\n",
    ">85で得た単語の意味ベクトルを読み込み，\"United States\"と\"U.S.\"のコサイン類似度を計算せよ．ただし，\"U.S.\"は内部的に\"U.S\"と表現されていることに注意せよ．\n",
    "\n",
    "#### 88. 類似度の高い単語10件\n",
    ">85で得た単語の意味ベクトルを読み込み，\"England\"とコサイン類似度が高い10語と，その類似度を出力せよ．\n",
    "\n",
    "#### 89. 加法構成性によるアナロジー\n",
    ">85で得た単語の意味ベクトルを読み込み，vec(\"Spain\") - vec(\"Madrid\") + vec(\"Athens\")を計算し，そのベクトルと類似度の高い10語とその類似度を出力せよ．\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第10章: ベクトル空間法 (II)\n",
    ">第10章では，前章に引き続き単語ベクトルの学習に取り組む．\n",
    "\n",
    "#### 90. word2vecによる学習\n",
    ">81で作成したコーパスに対してword2vecを適用し，単語ベクトルを学習せよ．さらに，学習した単語ベクトルの形式を変換し，86-89のプログラムを動かせ．\n",
    "\n",
    "#### 91. アナロジーデータの準備\n",
    ">単語アナロジーの評価データをダウンロードせよ．このデータ中で\": \"で始まる行はセクション名を表す．例えば，\": capital-common-countries\"という行は，\"capital-common-countries\"というセクションの開始を表している．ダウンロードした評価データの中で，\"family\"というセクションに含まれる評価事例を抜き出してファイルに保存せよ．\n",
    "\n",
    "#### 92. アナロジーデータへの適用\n",
    ">91で作成した評価データの各事例に対して，vec(2列目の単語) - vec(1列目の単語) + vec(3列目の単語)を計算し，そのベクトルと類似度が最も高い単語と，その類似度を求めよ．求めた単語と類似度は，各事例の末尾に追記せよ．このプログラムを85で作成した単語ベクトル，90で作成した単語ベクトルに対して適用せよ．\n",
    "\n",
    "#### 93. アナロジータスクの正解率の計算\n",
    ">92で作ったデータを用い，各モデルのアナロジータスクの正解率を求めよ．\n",
    "\n",
    "#### 94. WordSimilarity-353での類似度計算\n",
    ">The WordSimilarity-353 Test Collectionの評価データを入力とし，1列目と2列目の単語の類似度を計算し，各行の末尾に類似度の値を追加するプログラムを作成せよ．このプログラムを85で作成した単語ベクトル，90で作成した単語ベクトルに対して適用せよ．\n",
    "\n",
    "#### 95. WordSimilarity-353での評価\n",
    ">94で作ったデータを用い，各モデルが出力する類似度のランキングと，人間の類似度判定のランキングの間のスピアマン相関係数を計算せよ．\n",
    "\n",
    "#### 96. 国名に関するベクトルの抽出\n",
    ">word2vecの学習結果から，国名に関するベクトルのみを抜き出せ．\n",
    "\n",
    "#### 97. k-meansクラスタリング\n",
    ">96の単語ベクトルに対して，k-meansクラスタリングをクラスタ数k=5として実行せよ．\n",
    "\n",
    "#### 98. Ward法によるクラスタリング\n",
    ">96の単語ベクトルに対して，Ward法による階層型クラスタリングを実行せよ．さらに，クラスタリング結果をデンドログラムとして可視化せよ．\n",
    "\n",
    "#### 99. t-SNEによる可視化\n",
    ">96の単語ベクトルに対して，ベクトル空間をt-SNEで可視化せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
