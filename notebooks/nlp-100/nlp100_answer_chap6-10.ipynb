{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://www.cl.ecei.tohoku.ac.jp/nlp100/data/nlp.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 50\n",
    "import re\n",
    "sentence_sep = re.compile(r'(\\.|;|:|\\?|!) ([A-Z])')\n",
    "\n",
    "with open(\"./nlp.txt\") as f:\n",
    "    txt = f.read()\n",
    "txt = re.sub(sentence_sep, r'\\1\\n\\2', txt)\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 51\n",
    "def space2return(txt):\n",
    "    sentence_sep = re.compile(r'(\\.|;|:|\\?|!)\\n([A-Z])')\n",
    "    txt = re.sub(sentence_sep, r'\\1\\n\\n\\2', txt)\n",
    "    return re.sub(r' ', r'\\n', txt)\n",
    "\n",
    "txt = space2return(txt)\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 52\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def stem_text(txt):\n",
    "    for l in txt.split('\\n'):\n",
    "        yield l + '\\t' + ps .stem(l)\n",
    "    \n",
    "for line in stem_text(txt):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget http://nlp.stanford.edu/software/stanford-corenlp-full-2018-10-05.zip\n",
    "# !unzip stanford-corenlp-full-2018-10-05.zip\n",
    "!java -cp \"./stanford-corenlp-full-2018-10-05/*\" edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,parse,lemma,ner,coref -file ./nlp.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 53\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "tree = ET.parse(\"./nlp.txt.xml\")\n",
    "root = tree.getroot()\n",
    "for token in root.iter(\"token\"):\n",
    "    print(token.find(\"word\").text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 54\n",
    "for token in root.iter(\"token\"):\n",
    "    print(token.find(\"word\").text + \"\\t\" + token.find(\"lemma\").text + \"\\t\" + token.find(\"POS\").text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 55\n",
    "for token in root.iter(\"token\"):\n",
    "    NERtag = token.find(\"NER\").text\n",
    "    if NERtag == \"PERSON\":\n",
    "        print(token.find(\"word\").text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in root.find(\"document\"):\n",
    "    print(r.tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 56\n",
    "rep_dic_list = []\n",
    "\n",
    "# 辞書作り\n",
    "for coreferences in root.findall(\"document/coreference\"):\n",
    "    for mentions in coreferences:\n",
    "        for m in mentions:\n",
    "            if \"representative\" in m.attrib:\n",
    "                rep_txt = m.find(\"text\").text\n",
    "            else:\n",
    "                tmp_dic = {}\n",
    "                tmp_dic[\"sentence\"] = m.find(\"sentence\").text\n",
    "                tmp_dic[\"start\"] = m.find(\"start\").text\n",
    "                tmp_dic[\"end\"] = m.find(\"end\").text\n",
    "                tmp_dic[\"rep_txt\"] = rep_txt\n",
    "                rep_dic_list.append(tmp_dic)\n",
    "                \n",
    "# 出力\n",
    "for s in root.iter(\"sentence\"):\n",
    "    rep_sent_list = [rd for rd in rep_dic_list if rd[\"sentence\"] == s.attrib[\"id\"]]\n",
    "    # 置換が必要な文かどうか\n",
    "    if len(rep_sent_list) == 0:\n",
    "            print(\" \".join([token.find(\"word\").text for token in s.iter(\"token\")]), end=\" \")\n",
    "    else:\n",
    "        for token in s.iter(\"token\"):\n",
    "            tid = token.attrib[\"id\"]\n",
    "            rep_token_list = [rd for rd in rep_sent_list if rd[\"start\"] == tid or rd[\"end\"] == tid]\n",
    "            \n",
    "            if len(rep_token_list) > 0:\n",
    "                # 該当は１つなので取り出す\n",
    "                rep_dic = rep_token_list[0]\n",
    "                \n",
    "                #　装飾\n",
    "                if tid == rep_dic[\"start\"]:\n",
    "                    print(\"「\" + rep_dic[\"rep_txt\"] + \" (\", end=\" \")\n",
    "                if tid == rep_dic[\"end\"]:\n",
    "                    print(\")」\", end=\" \")\n",
    "                    \n",
    "            print(token.find(\"word\").text, end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 57\n",
    "import random, pathlib\n",
    "from graphviz import Digraph\n",
    "\n",
    "f = pathlib.Path('nlp.png')\n",
    "fmt = f.suffix.lstrip('.')\n",
    "fname = f.stem\n",
    "\n",
    "dot = Digraph(format=fmt)\n",
    "dot.attr(\"node\", shape=\"circle\")\n",
    "\n",
    "sent_id = 3\n",
    "\n",
    "for sents in root.findall(f\"document/sentences/sentence[@id='{sent_id}']\"):\n",
    "    for deps in sents:\n",
    "        for dep in deps.findall(\"[@type='collapsed-dependencies']\"):\n",
    "            for token in dep:\n",
    "                gvnr = token.find(\"governor\")\n",
    "                dpnt = token.find(\"dependent\")\n",
    "                dot.node(gvnr.attrib[\"idx\"], gvnr.text)\n",
    "                dot.node(dpnt.attrib[\"idx\"], dpnt.text)\n",
    "                dot.edge(gvnr.attrib[\"idx\"], dpnt.attrib[\"idx\"])\n",
    "\n",
    "\n",
    "dot.filename = fname\n",
    "dot.render()\n",
    "\n",
    "# print(dot)\n",
    "from IPython.display import Image, display_png\n",
    "display_png(Image(str(f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 58\n",
    "for sents in root.findall(f\"document/sentences/sentence\"):\n",
    "    for deps in sents:\n",
    "        for dep in deps.findall(\"[@type='collapsed-dependencies']\"):\n",
    "            nsubj_list = []\n",
    "            for token in dep.findall(\"./dep[@type='nsubj']\"):\n",
    "                gvnr = token.find(\"governor\")\n",
    "                dpnt = token.find(\"dependent\")\n",
    "                nsubj_list.append( {\n",
    "                    (gvnr.attrib[\"idx\"], gvnr.text): (dpnt.attrib[\"idx\"], dpnt.text)\n",
    "                })\n",
    "            for token in dep.findall(\"./dep[@type='dobj']\"):\n",
    "                gvnr = token.find(\"governor\")\n",
    "                dpnt = token.find(\"dependent\")\n",
    "                dobj_tuple = (gvnr.attrib[\"idx\"], gvnr.text)\n",
    "                \n",
    "                if dobj_tuple in [list(nsubj.keys())[0] for nsubj in nsubj_list]:\n",
    "                    idx =  [list(nsubj.keys())[0] for nsubj in nsubj_list].index( dobj_tuple )\n",
    "                    jutugo = gvnr.text\n",
    "                    shugo = nsubj_list[idx][dobj_tuple][1]\n",
    "                    mokutekigo = dpnt.text\n",
    "                    print(shugo + \"\\t\" + jutugo + \"\\t\" + mokutekigo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_nest(t):\n",
    "    if isinstance(t[0], str):\n",
    "        if isinstance(t[1], str):\n",
    "            if t[0] == \"NP\":\n",
    "                print(t[1])\n",
    "            return t[1]\n",
    "        else:\n",
    "            if t[0] == \"NP\":\n",
    "                np_list = []\n",
    "                for i in t[1:]:\n",
    "                    res = search_nest(i)\n",
    "                    if isinstance(res, str):\n",
    "                        np_list.append(search_nest(i))\n",
    "                if len(np_list) > 0:\n",
    "                    print(' '.join(np_list))\n",
    "            else:\n",
    "                for i in t[1:]:\n",
    "                    search_nest(i)\n",
    "    else:\n",
    "        for i in t:\n",
    "            search_nest(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 59 \n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "\n",
    "tree = ET.parse(\"./nlp.txt.xml\")\n",
    "root = tree.getroot()\n",
    "sent_id = 30\n",
    "\n",
    "for parse in root.findall(f\"document/sentences/sentence[@id='{sent_id}']/parse\"):\n",
    "    S_str = parse.text\n",
    "    S_str = S_str.replace(\"(\", \"('\")\n",
    "    S_str = S_str.replace(\")\", \"')\")\n",
    "    S_str = S_str.replace(\" \", \"', '\")\n",
    "    S_str = S_str.replace(\"'(\", \"(\")\n",
    "    S_str = S_str.replace(\")'\", \")\")\n",
    "    exec(f\"S_tuple = {S_str[:-2]}\")\n",
    "    search_nest(S_tuple)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
